# 现代程序设计第十三次作业



## 作业要求

​	**协程有时被称为“微线程”，因为二者有类似的使用场景。和线程相比，协程占用资源更少、切换迅速，且实现简单（如协程是协作式调用，一般不用考虑资源的抢占，所以大部分情况下不需要通过同步原语来避免冲突）。通常协程被用在高并发的IO场景中，因此本周要求在第十二次作业的基础上，利用协程重写并扩充已有爬虫的功能**



### 相关库的导入

```python
from multiprocessing import Queue
from asyncio.runners import run
import requests
from bs4 import BeautifulSoup
import re
from threading import Thread, currentThread
import queue
import urllib.request
import time
import aiofiles
import asyncio
import aiohttp
```



### 数据初始化

```python
save_path = 'info/'
img_path = 'cover/'
produce_num = 0
consumer_num = 0

headers_1 = {
            'Connection': 'close',
            'Referer': 'https://music.163.com/',
            'Host': 'music.163.com',
            'cookie':'',
            "Accept-Encoding": "identity",
            "Accept": "",
            "user-agent":''
           }

headers_2 = {
            'Connection': 'close',
            'Referer': 'https://music.163.com/',
            'Host': 'music.163.com',
            'cookie':'',
            "Accept-Encoding": "identity",
            "Accept": "",
            "user-agent":''
           }

```





### 获取id和歌单url

```python
# 编写爬虫函数
async def get_id(url):
    async with aiohttp.ClientSession(headers = headers_1, connector=aiohttp.TCPConnector(ssl=False)) as session:
        async with session.get(url) as response:
            return await response.text()
          
# 协程爬取id
async def run_get_id(pages):
    print(f'the no{currentThread().name} process_id has started')
    #for i in pages:
    for i in range(1):
        #time.sleep(1)
        path = 'https://music.163.com'
        url = 'https://music.163.com/discover/playlist/?order=hot&cat=说唱&limit=35&offset=' + str(pages)
        #try:
        '''while True:
                #response = requests.session().get(url=url,headers=headers_1)
                response = await get_id(url)
                time.sleep(1)
                if response != []:
                    break'''           
            #response = requests.session().get(url=url,headers=headers_1)
            #response = await get_id(url)
            #time.sleep(1)
        #except:
            #print('网络请求错误')
        '''while True:
                #response = requests.session().get(url=url,headers=headers_1)
                response = await get_id(url)
                time.sleep(1)
                if response != []:
                    break'''           
            #response = requests.session().get(url=url,headers=headers_1)
        #response = await get_id(url)
        html = await get_id(url)
            #time.sleep(1)
        #time.sleep(1)
        #html = await response.text()
        #html = response.text()

        playlist_ids = []
        playlist_ids.extend(re.findall(r'playlist\?id=(\d+?)" class="msk"',html))
        #print(playlist_ids)

        soup = BeautifulSoup(html, 'html.parser')

        #test = soup.find('ul')
        #print(test)
        text = soup.find('ul',class_='m-cvrlst f-cb')
        #print(len(text))
        #print(text)
        id_list = []
        id_text = soup.select(".dec a")
        for i in range(len(id_text)):
            id_list.append(id_text[i]['href'])
        print(id_list)

        #print(title)

        #title_list = soup.select("#m-pl-container li")
        #print(title_list)
        q.put(id_list)
```



### 爬取歌单信息

```python
def download_img(path,title,src):
    title = title.replace('/',' ')
    with open(path+title+'.jpg','wb+') as f:
        res = urllib.request.urlopen(src)
        img = res.read()
        f.write(img)
# 编写爬虫函数
async def get_id(url):
    async with aiohttp.ClientSession(headers = headers_1, connector=aiohttp.TCPConnector(ssl=False)) as session:
        async with session.get(url) as response:
            return await response.text()

# 协程爬取id
async def run_get_id(pages):
    print(f'the no{currentThread().name} process_id has started')
    #for i in pages:
    for i in range(1):
        #time.sleep(1)
        path = 'https://music.163.com'
        url = 'https://music.163.com/discover/playlist/?order=hot&cat=说唱&limit=35&offset=' + str(pages)
        #try:
        '''while True:
                #response = requests.session().get(url=url,headers=headers_1)
                response = await get_id(url)
                time.sleep(1)
                if response != []:
                    break'''           
            #response = requests.session().get(url=url,headers=headers_1)
            #response = await get_id(url)
            #time.sleep(1)
        #except:
            #print('网络请求错误')
        '''while True:
                #response = requests.session().get(url=url,headers=headers_1)
                response = await get_id(url)
                time.sleep(1)
                if response != []:
                    break'''           
            #response = requests.session().get(url=url,headers=headers_1)
        #response = await get_id(url)
        html = await get_id(url)
            #time.sleep(1)
        #time.sleep(1)
        #html = await response.text()
        #html = response.text()

        playlist_ids = []
        playlist_ids.extend(re.findall(r'playlist\?id=(\d+?)" class="msk"',html))
        #print(playlist_ids)

        soup = BeautifulSoup(html, 'html.parser')

        #test = soup.find('ul')
        #print(test)
        text = soup.find('ul',class_='m-cvrlst f-cb')
        #print(len(text))
        #print(text)
        id_list = []
        id_text = soup.select(".dec a")
        for i in range(len(id_text)):
            id_list.append(id_text[i]['href'])
        print(id_list)

        #print(title)

        #title_list = soup.select("#m-pl-container li")
        #print(title_list)
        q.put(id_list)

# 爬取歌单信息
async def get_info(url,headers):
    async with aiohttp.ClientSession(headers = headers, connector=aiohttp.TCPConnector(ssl=False)) as session:
        async with session.get(url) as response:
            return await response.text()

async def run_get_info():
    print(f'the no{currentThread().name} pconsumer has started')
    while True:
        id_list = q.get()
        print(id_list)
        if id_list == []:
            break
        path = 'https://music.163.com'
        for i in range(len(id_list)):
            time.sleep(1)
            id = id_list[i][-10:]
            url = path+id_list[i]
            try:
                count = 1
                while True:
                    start = time.time()
                    '''response = requests.session().get(url=url,headers=headers_1)
                    html = response.content.decode('utf-8')'''
                    html = await get_info(url,headers_1)
                    soup = BeautifulSoup(html, 'html.parser')
                    time.sleep(count)
                    if soup.find('p',class_='intr f-brk') == None:
                        break
                    '''response = requests.session().get(url=url,headers=headers_2)
                    html = response.content.decode('utf-8')'''
                    html = await get_info(url,headers_2)
                    soup = BeautifulSoup(html, 'html.parser')
                    end = time.time()
                    time.sleep(count)
                    if soup.find('p',class_='intr f-brk') == None:
                        break
                    count += 1
                    if count > 2:
                        print('无法请求到结果')
                        break
                    if end-start > 2:
                        print('超时')
                        break
            except:
                print(soup.find('p',class_='intr f-brk'))
                print('网络超时')
            #html = response.content.decode('utf-8')
            #soup = BeautifulSoup(html, 'html.parser')
            #print(soup)

            # 歌单的封面图片（需把图片保存到本地）、歌单标题、创建者id、创建者昵称、介绍、歌曲数量、播放量、添加到播放列表次数、分享次数、评论数。
            try:
                # 歌单标题
                title = soup.find('h2',class_ = 'f-ff2 f-brk').text
                #print(title)

                # 图片下载
                #img_info = soup.find('div',class_='cover u-cover u-cover-dj')
                #img_url = img_info.find('img',class_='j-img')['data-src']
                #img_id = id_list[i][-10:]

                #download_img(img_path,title,img_url)
                #print(img_url)

                # 创建者昵称
                create_name = soup.find('span',class_='name').text.strip('\n')
                #print(create_name)
                
                # 创建者id
                create_id = soup.find('span',class_='name').select('a')[0]['href'][14:]
                #print(create_id)

                # 介绍
                describe = soup.find('p',class_='intr f-brk').text.replace('\n','\t').replace(',','，')
                #print(describe)

                # 歌曲数量
                song_num = soup.find('span','sub s-fc3').find('span').text
                #print(song_num)

                # 播放量
                play_num = soup.find('strong','s-fc6').text
                #print(play_num)

                # 添加到播放列表次数
                add_num = soup.find('a','u-btni u-btni-fav')['data-count']
                #print(add_num)

                # 分享次数
                share_num = soup.find('a','u-btni u-btni-share')['data-count']
                #print(share_num)

                # 评论数
                comment_num = soup.find('a','u-btni u-btni-cmmt').find('span').text
                #print(comment_num)

                print('success')

                #text_q.put(id+','+title+','+create_name+','+create_id+','+str(describe)+','+song_num+','+play_num+','+add_num+','+share_num+','+comment_num+'\n')
                text = id+','+title+','+create_name+','+create_id+','+str(describe)+','+song_num+','+play_num+','+add_num+','+share_num+','+comment_num+'\n'
                with open(save_path+'说唱.csv','a+') as f:
                    f.write(text)
            except:
                print('failed')
            '''with open(save_path+'说唱.csv','a+') as f:
                f.write(id+','+title+','+create_name+','+create_id+','+str(describe)+','+song_num+','+play_num+','+add_num+','+share_num+','+comment_num+'\n')
'''
def write():
    while True:
        text = text_q.get()
        if text == None:
            break
        with open(save_path+'说唱.csv','a+') as f:
            f.write(text)
```



### 主函数

```python
if __name__ == '__main__':

    q = queue.Queue()
    text_q = queue.Queue()
    producer_list = []
    consumer_list = []
    with open(save_path+'说唱.csv','a+') as f:
        f.write('歌曲ID'+','+'歌单标题'+','+'创建者昵称'+','+'创建者id'+','+'介绍'+','+'歌曲数量'+','+'播放量'+','+'添加到播放列表次数'+','+'分享次数'+','+'评论数'+'\n')

    loop_id = asyncio.get_event_loop()
    sheet = list(range(0,1300,35))
    tasks = [run_get_id(sheet[i]) for i in range(len(sheet))]    #len(sheet)
    loop_id.run_until_complete(asyncio.wait(tasks))

    loop_info = asyncio.get_event_loop()
    thread_num = 38
    tasks = [run_get_info() for i in range(thread_num)]    #
    loop_info.run_until_complete(asyncio.wait(tasks))

    '''write_t = Thread(target=write)
    write_t.start()
    write_t.join()'''
    
    #asyncio.run(run_get_id(sheet))
    '''p_t = Thread(target=run_get_id)
    p_t.start()
    p_t.join()
    thread_num = 1'''
    '''num = len(sheet)//thread_num

    for i in range(thread_num-1):
        p_t = produce_id(sheet[num*i:num*(i+1)],q)
        producer_list.append(p_t)
    p_t = produce_id(sheet[num*(thread_num-1):],q)
    producer_list.append(p_t)'''

    #for i in range(thread_num):
        #c_t = ''
        #consumer_list.append(c_t)

    '''for p in producer_list:
        p.start()
    for p in producer_list:
        p.join()'''
    #for c in consumer_list:
        #c.start()
    #p_t.join()
    #for c in consumer_list:
        #c.join()


    '''produce_thread = produce_id([0],q)
    produce_thread.start()
    produce_thread.join()

    consumer_thread = consumer(q)
    consumer_thread.start()
    consumer_thread.join()'''
```



### 运行结果

<img src="/Users/xihe/Library/Application Support/typora-user-images/image-20211229211618012.png" alt="image-20211229211618012" style="zoom:50%;" />

<img src="/Users/xihe/Library/Application Support/typora-user-images/image-20211229211642433.png" alt="image-20211229211642433" style="zoom:50%;" />

<img src="/Users/xihe/Library/Application Support/typora-user-images/image-20211229211735296.png" alt="image-20211229211735296" style="zoom:50%;" />

<img src="/Users/xihe/Library/Application Support/typora-user-images/image-20211229211755985.png" alt="image-20211229211755985" style="zoom:50%;" />

